---
title: "Доказательство ЦПТ последовательной подменой слагаемых"
format:
  html:
    toc: true
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    include-in-header: 
      text: |
        \usepackage{physics}
        \usepackage{tikz}
bibliography: references.bib
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

::: {.hidden}
$$
 \newcommand\cN{{\mathcal{N}}}
 \newcommand\P{{\mathbb{P}}}
 \newcommand\E{{\mathbb{E}}}
 \DeclareMathOperator{\plim}{plim}
 \newcommand\Var{{\mathrm{Var}}}
 \newcommand\Cov{{\mathrm{Cov}}}
 \newcommand\Corr{{\mathrm{Corr}}}
$$
:::




:::{.callout-note}
## Лемма

Если для любой трижды дифференцируемой действительной функций $h$ с ограниченными тремя производными выполнено условие 
$$
\lim_{n\to\infty} \E(h(R_n)) = \E(h(R)),
$$
то последовательность случайных величин $(R_n)$ сходится к $R$ по распределению, то есть,
$$
\lim_{n\to\infty} \P(R_n \leq x) = \P(R\leq x),
$$
для любой точки $x$, в которой функция распределения $F$ величины $R$ непрерывна. 
:::



:::{.callout-caution collapse="true"}
## Доказательство леммы

Нам надо доказать, что при большом $n$ вероятность $\P(R_n\leq x)$ не может слишком сильно отличаться от вероятности $\P(R \leq x)$ ни в большую, ни в меньшую сторону. 

Докажем половину утверждения, вторая половина доказывается по аналогии. Основная идея доказательства такова: вероятность $\P(R_n \leq x)$ можно заменить на ожидание $\E(I(R_n \leq x))$, а «ступенчатый» индикатор $I$ можно сколь угодно точно приблизить гладкой много раз дифференцируемой функцией. 

Поехали. Выбираем произвольное положительное $\varepsilon$. Наша цель — доказать, что начиная с некоторого $n$ вероятность $\P(R_n \leq x) > \P(R \leq x) - \varepsilon$. 

С помощью ожидания индикатора и функции распределения $F$ величины $R$ наша цель записывается так:
$$
\E(I(R_n \leq x)) > F(x) - \varepsilon.
$$

Отступим от точки $x$ чуть-чуть влево, в точку $x - \delta$. В силу непрерывности $F$ в точке $x$ размер оступа $\delta$ можно выбрать так, что $F(x-\delta) > F(x) - \varepsilon/2$. 

Теперь придумаем трижды дифференцируемую функцию $h$, которая чуть-чуть занижает индикатор $I(R_n \leq x)$. А именно, левее $x-\delta$ функция $h$ равна 1, правее $x$ функция $h$ равна нулю, а на отрезке $[x-\delta, x]$ функция $h$ плавно спускается от 1 к 0. По построению, 
$$
I(R_n \leq x - \delta) \leq h(R_n) \leq I(R_n \leq x).
$$
Делаем первый шаг по замене индикатора на не превосходящую его гладкую функцию $h$: 
$$
\P(R_n\leq x) = \E(I(R_n \leq x)) \geq \E(h(R_n)).
$$
Теперь выберем $n$ достаточно большим, так, чтобы 
$$
\E(h(R_n)) \geq \E(h(R)) - \varepsilon/2. 
$$
Теперь заменяем гладкую функцию $h$ на не превосходящий её индикатор,
$$
\E(h(R)) - \varepsilon/2 \geq \E(I(R \leq x-\delta)) - \varepsilon/2 = F(x-\delta) - \varepsilon/2.
$$
Вспоминаем, что точку $x-\delta$ мы выбрали недалеко от $x$ и получаем в итоге, что начиная с некоторого $n$
$$
\P(X_n \leq x) \geq F(x) - \varepsilon.
$$
Аналогично доказывается и вторая половина. На этот раз надо отступать от $x$ вправо в точку $x+\delta$, и заменять индикатор $I(R_n \leq x)$ мажорирующей его гладкой функцией $h$.  
:::



:::{.callout-caution collapse="true"}
## Упражнение к лемме

Докажите, что для любого $\varepsilon$ начиная  с некоторого $n$ выполнено неравенство 

$$
\P(X_n \leq x) \leq F(x) + \varepsilon.
$$

:::

:::{.callout-note}
## Центральная предельная теорема

Если величины $Q_1$, $Q_2$, \ldots, независимы и одинаково распределены с конечным ожиданием $\mu$ и дисперсией $\sigma^2$, то отмасштабированная сумма
$$
Z_n = \frac{\sum_{i=1}^n Q_i - \E(\sum_{i=1}^n Q_i)}{\sqrt{\Var(\sum_{i=1}^n Q_i)}}
$$
стремится по распределению к $\cN(0;1)$.
:::


Доказательство. 
Для начала представим $S_n$ в виде отмасштабированных слагаемых. 
$$
Z_n = \frac{Q_1 - \mu}{\sigma \sqrt n} + \ldots + \frac{Q_{n-1} - \mu}{\sigma \sqrt n} + \frac{Q_n - \mu}{\sigma \sqrt n} = X_1 + \ldots + X_{n-1} + X_n
$$
Замечаем, что $\E(X_i) = 0$, $\Var(X_i) = 1/n$.

Теперь потихоньку начнем менять слагаемые в правом хвосте на независимые слагаемые $Y_i$ с таким же нулевым ожиданием, такой же дисперсией $1/n$, но нормально распределенные:

Удалим $X_n$, добавим $Y_n$, удалим $X_{n-1}$, добавим $Y_{n-1}$, и так далее...

Промежуточную сумму до удаления очередного $X_i$ обозначим с помощью $Z_{n,i}$, а после удаления очередного $X_i$ — с помощью $S_{n,i}$.

Для трёх величин схема выглядит так:
$$
X_1 + X_2 + X_3 = Z_{3,3} \overset{-X_3}{\longrightarrow}S_{3,3}\overset{+Y_3}{\longrightarrow}Z_{3,2}\overset{-X_2}{\longrightarrow}S_{3,2}\overset{+Y_2}{\longrightarrow}Z_{3,1}\overset{-X_1}{\longrightarrow}S_{3,1}\overset{+Y_1}{\longrightarrow}Z_{3,0}=Y_1 + Y_2 + Y_3
$$

Величина $Z_{n,i}$ будет своими первыми $i$ слагаемыми содержать иксы, а оставшимися слагаемыми — игреки. В сумме $S_{n,i}$ полностью отсутствует $i$-е слагаемое, слагаемые с меньшими номерами — это $X_1$, \ldots, $X_{i-1}$, слагаемые с большими номерами — это $Y_{i+1}$, \ldots, $Y_n$.

Для наглядного примера,
$$
Z_{5, 3} = X_1 + X_2 + X_3 + Y_4 + Y_5,
$$
$$
S_{5, 3} = X_1 + X_2 + 0 + Y_4 + Y_5,
$$

В общем виде схема выглядит так:
$$
\sum_{i=1}^n X_i = Z_{n,n} \overset{-X_n}{\longrightarrow}S_{n,n}\overset{+Y_n}{\longrightarrow}Z_{n,n-1}\overset{-X_{n-1}}{\longrightarrow} \ldots \overset{-X_2}{\longrightarrow}S_{n,2}\overset{+Y_2}{\longrightarrow}Z_{n,1}\overset{-X_1}{\longrightarrow}S_{n,1}\overset{+Y_1}{\longrightarrow}Z_{n,0}=\sum_{i=1}^n Y_i
$$
В схеме $n$ шагов, каждый из которых состоит из двух полушагов, удаления $X_i$ и добавления $Y_i$.

Заметим, что $S_{n,i}$ не зависит ни от $X_i$, ни от $Y_i$. Это пригодится. 

Замечаем также, что $Z_{n,0} = Y_1 + \ldots + Y_n \sim \cN(0;1)$.

В силу леммы нам достаточно доказать, что для любой трижды дифференцируемой $h$ с ограниченными тремя производными $\E(h(Z_{n,n})) \to \E(h(Z_{n,0}))$.

Поехали. Выбираем произвольное положительное $\varepsilon$. Наша цель — доказать, что начиная с некоторого $n$ отличие этих двух ожиданий невелико,
$$
\abs{\E(h(Z_{n,n})) - \E(h(Z_{n,0}))} < \varepsilon.
$$

Посмотрим на нашу схему подмен
$$
h\left(\sum_{i=1}^n X_i\right)=h(Z_{n,n}) \overset{-X_n}{\longrightarrow}h(S_{n,n})\overset{+Y_n}{\longrightarrow}h(Z_{n,n-1})\overset{-X_{n-1}}{\longrightarrow} \ldots \overset{+Y_2}{\longrightarrow}h(Z_{n,1})\overset{-X_1}{\longrightarrow}h(S_{n,1})\overset{+Y_1}{\longrightarrow}h(Z_{n,0})=h\left(\sum_{i=1}^n Y_i\right)
$$


С ростом $n$ цепочка растет, а каждый шаг по идее должен становится всё меньше. 
Если мы докажем, что начиная с некоторого $n$ разница
$$
\abs{\E(h(Z_{n,i})) - \E(h(Z_{n,i-1}))}
$$ 
от **каждого** шага становится меньше $\varepsilon/n$, то дело будет в шляпе! 

Остановимся на первой паре полушагов, 
$$
h(Z_{n,n}) \overset{-X_n}{\longrightarrow}h(S_{n,n})\overset{+Y_n}{\longrightarrow}h(Z_{n,n-1})
$$
Доказательство для других пар полушагов полностью аналогично. 

Наша разница $h(Z_{n,n}) - h(Z_{n,n-1})$ разбивается в два полушага,
$$
\E h(Z_{n,n}) - \E h(Z_{n,n-1}) = \left(\E h(Z_{n,n}) - \E h(S_{n,n}) \right) - \left(\E h(Z_{n,n-1}) - \E h(S_{n,n}) \right).
$$

Оказывается, что обе промежуточных разницы, $\left(\E h(Z_{n,n}) - \E h(S_{n,n}) \right)$ и $\left(\E h(Z_{n,n-1}) - \E h(S_{n,n}) \right)$, очень похожи на константу $\E\left(\frac{h''(S_{n,n})}{2!}\right)$. И начиная с некоторого $n$ отличие каждой промежуточной разницы от этой константы будет меньше $\varepsilon/2n$. При взятии разницы константа $\E\left(\frac{h''(S_{n,n})}{2!}\right)$ уничтожится, и разница для целого шага окажется меньше $\varepsilon/n$. 

Проведем доказательство для первой из них, $\left(\E h(Z_{n,n}) - \E h(S_{n,n}) \right)$. Доказательство для второй $\left(\E h(Z_{n,n-1}) - \E h(S_{n,n}) \right)$ аналогично. 

В этот момент можно уже не писать индекс $(n,n)$ у $Z$ и $S$ :)

Выполним линеаризацию функции $h(Z_{n,n})$ в окрестности точки $S_{n,n}$. Заметим предварительно, что эти точки отличаются ровно на $X_n$, $Z_{n,n} = S_{n,n} + Z_n$.
$$
h(Z_{n,n}) \approx h(S_{n,n}) + h'(S_{n,n})(Z_{n,n} - S_{n,n}) = h(S_{n,n}) + h'(S_{n,n}) X_n.
$$
Для доказательства потребуется вспомнить точный смысл примерного равенства, а именно, остаток в форме Лагранжа. Найдётся такая точка $C$ между $S_{n,n}$ и $Z_{n,n}$, что 
$$
h(Z_{n,n}) = h(S_{n,n}) + h'(S_{n,n})X_n + \frac{h''(C)}{2!}X_n^2.
$$
Выделяем нужную нам разницу,
$$
h(Z_{n,n}) - h(S_{n,n}) = h'(S_{n,n})X_n + \frac{h''(C)}{2!}X_n^2.
$$
Прибавим и вычтем справа $h''(S_{n,n})$,
$$
h(Z_{n,n}) - h(S_{n,n}) = h'(S_{n,n})X_n + \frac{h''(S_{n,n})}{2!}X_n^2 + \frac{h''(C) - h''(S_{n,n})}{2!}X_n^2.
$$

Берём математическое ожидание, вспомнив, что $\E(X_n) =0$, $\Var(X_n)= 1/n$, а $X_n$ не зависит от $S_n$,
$$
\E h(Z_{n,n}) - \E h(S_{n,n}) = 0 + \E\left(\frac{h''(S_{n,n})}{2!}\right) + \E\left(\frac{h''(C) - h''(S_{n,n})}{2!}X_n^2\right).
$$
Сосредоточимся на последнем слагаемом и рассмотрим два случая, в зависимости от того, больше ли $\abs X_n$ чем $\delta$.
$$
\frac{h''(C) - h''(S_{n,n})}{2!}X_n^2 = \left(\frac{h''(C) - h''(S_{n,n})}{2!}X_n^2I(\abs{X_n} \leq \delta)\right) + \left(\frac{h''(C) - h''(S_{n,n})}{2!}X_n^2I(\abs{X_n} > \delta)\right).
$$
Изучаем первое слагаемое. Вспомним, что точка $C$ находится между $S_{n,n}$ и $Z_{n,n}$, а $S_{n,n} + X_n = Z_{n,n}$.
Поэтому $\abs{C - S_{n,n}} \leq \delta$, если $\abs{X_n}\leq \delta$. 

У функции $h$ ограничена третья производная, выберем $\delta$ настолько маленьким, чтобы зажать разницу $h''(C) - h''(S_{n,n})$ до величины меньшей $\varepsilon/2$.

Получаем ограничение для первого слагаемого,
$$
\E\abs\left(\frac{h''(C) - h''(S_{n,n})}{2!}X_n^2I(\abs{X_n} \leq \delta)\right) \leq E\left(\frac{\varepsilon}{4}X_n^2\right)=\frac{\varepsilon}{4n}
$$
Изучаем второе слагаемое. У функции $h$ ограничена вторая производная константой $M$.

$$
\E\abs\left(\frac{h''(C) - h''(S_{n,n})}{2!}X_n^2I(\abs{X_n} > \delta)\right) \leq E\left(\frac{M + M}{4}X_n^2I(\abs{X_n} > \delta)\right)=\frac{M}{2n}\E(X_n^2I(\abs{X_n} > \delta))
$$

Подберем $n$ настолько большим, что $\E(X_n^2 I(\abs{X_n} > \delta)) < \varepsilon/2M$. 


Вспомним наш долгий путь. Сначала мы разбили разницу $h(Z_{n,n}) - h(Z_{n,0})$ на $2n$ полушагов. Каждый шаг состоит из полушага удаления $X_i$ и полушага добавления $Y_i$. Затем мы рассмотрели для каждого полушага два подслучая, в зависимости от того, что больше $\abs X_i$ или $\delta$. 

## Источники

В основном изложение следует статье [@Chin2022ASA]. Постарался сделать изложение более «мотивированным», чтобы перед шагами яснее была видна цель. Также излагаю один случай из повторяющихся. С одной стороны, это облегчает понимание, с другой стороны аналогичный случай можно решать в виде упражнения. 

