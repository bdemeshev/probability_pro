\documentclass[11pt,russian,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Linux Libertine O}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
\usepackage{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{unicode=true,
            pdftitle={Две лекции про Байесовский подход},
            pdfauthor={Винни-Пух :)},
            colorlinks=true,
            linkcolor=blue,
            citecolor=blue,
            urlcolor=blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=russian]{babel}
\else
  \usepackage{polyglossia}
  \setmainlanguage[]{russian}
\fi
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Две лекции про Байесовский подход}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Винни-Пух :)}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{10/9/2017}

\newfontfamily{\cyrillicfonttt}{Linux Libertine O}
\newfontfamily{\cyrillicfont}{Linux Libertine O}
\newfontfamily{\cyrillicfontsf}{Linux Libertine O}

\begin{document}
\maketitle

\section{Байесовский подход}\label{-}

В классическом методе максимального правдоподобия неизвестный параметр
\(\theta\) - это константа. А в байесовском подходе \(\theta\) - это
ненаблюдаемая случайная величина.

А наблюдения, \(Y_1\), \(Y_2\), \ldots, \(Y_n\), и в классическом
частотном подходе, и в байесовском считаются наблюдаемыми случайными
величинами.

В байесовском подходе можно выделить два предположения:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Изначальное мнение о неизвестном параметре \(\theta\),
  сформулированное в виде закона распределения. Этот закон называется
  априорным законом распределения.
\item
  Функция правпдоподобия - представление о законе распределения
  наблюдений \(Y_1\), \(Y_2\), \ldots, \(Y_n\).
\end{enumerate}

Если к этим двум предположениям добавить сами наблюдения \(Y_1\),
\(Y_2\), \ldots, \(Y_n\), то по формуле условной вероятности можно
получить вывод:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Мнение о неизвестном параметре \(\theta\) с учётом полученных
  наблюдений. Это апостериорный закон распределения.
\end{enumerate}

Байесовский подход - это модно, стильно и молодёжно!

Фейсбук прогнозирует дневные данные с помощью байесовского алгоритма
\href{https://facebook.github.io/prophet/}{facebook.github.io/prophet/}.

А вот целая летняя школа:
\href{https://habr.com/post/337028/}{habr.com/post/337028/}.

И да, пара вопросиков в экзамен войдёт :)

\subsection{Первая задача}\label{-}

Пример. Подходит ко мне в поезде подозрительный тип и говорит, ``а давай
сыграем в орлянку моей монеткой''. Здесь явно возникает неизвестный
параметр \(p\) - вероятность выпадения орла.

Предположим, что:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Изначально я ничего не знаю про этого типа, может он любит делать
  монетки, чаще выпадающие орлом, а может и нет :) Поэтому буду считать,
  что изначально вероятность выпадения орла равномерна на отрезке
  \([0;1]\), \(p \sim U[0;1]\).
\item
  Если бы \(p\) было известно и фиксировано, то результаты подбрасывания
  монетки, \(Y_i\), были независимы, и \(P(Y_i = \text{Орёл})=p\).
\end{enumerate}

Пока подозрительный тип отвернулся, я успел три раза подбросить монетку
и получил выборку \(Y_1=\text{Орёл}\), \(Y_2=\text{Решка}\),
\(Y_3=\text{Орёл}\).

Каким должно быть моё мнение о монетке с учётом полученной информации?

Чему, например, равна вероятность \(P(p>0.5 \mid \text{Данные})\)?

Чему равно \(E(p \mid \text{Данные})\)?

Перейдём к решению!

\subsubsection{Маленькое обозначение :)}\label{-}

Для удобства мы введём значок ``пропорционально'' \(\propto\).

Например, формула \[
f(x) \propto x^2
\] означает, что функция \(f(x)\) может равняться \(5x^2\) или
\(19x^2\), но никак не \(\cos(x)+4\).

\ldots{}.

В качестве основной формулы получаем:

\[
f(\theta \mid \text{Данные}) \propto f(\theta) \cdot f(\text{Данные} \mid \theta)
\]

Словами: \[
\text{Апостериорная плотность} \propto \text{Априорная плотность} \cdot \text{Функция правдоподобия}
\]

\ldots{}

Получаем апостериорную функцию плотности \[
f(p \mid \text{Данные }) =
\begin{cases}
12p^2 (1-p), \text{ если } p \in [0;1] \\
0, \text{ иначе}
\end{cases}
\]

С помощью неё легко найти \(P(p>0.5 \mid \text{Данные})\) и
\(E(p \mid \text{Данные})\).

\section{Соответствия}

Некоторые вопросы, на которые байесовский подход даёт ответы, например,
чему равна вероятность \(P(p>0.5 \mid \text{Данные})\) бессмысленны в
классическом подходе. Ведь в классическом подходе \(p\) - неизвестная
константа и потому любая вероятность связанная с истинным параметром
либо равна 0, либо 1, а чему конкретно - узнать невозможно.

В классическом подходе находят точечные оценки и интервальные оценки
неизвестных параметров. Что заменяет их в байесовском подходе?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Точечную оценку \(\hat\theta_{ML}\) обычно заменяют на:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  апостериорное среднее
  \(\hat\theta_{BA}=E(\theta \mid \text{Данные})\);
\item
  апостериорную моду
  \(\hat\theta_{MAP}=Mode(\theta \mid \text{Данные})\), MAP
  расшифровывается как maximum aposteriori estimator.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Интервальные оценки в байесовском подходе строят естественным образом:
  находят такие точки \(a\) и \(b\), что
  \(P(\theta \in [a;b] \mid \text{Данные}) = 0.95\). При этом есть два
  популярных варианта:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  симметрично по вероятности интервалы: слева от \(a\) оказывается такая
  же вероятность, как справа от \(b\);
\item
  HPD-интервал (highest probability density): для распределения с одной
  модой интервал строится так, чтобы плотность на отрезке \([a;b]\) была
  выше, чем за отрезком \([a;b]\).
\end{itemize}

Если апостериорное распределение не симметричное, то HPD интервал может
оказаться существенно короче симметричного по вероятности. А если
апостериорное распределение бимодальное, то HPD-интервал вовсе и не
интервал :), а будет состоять из двух кусков.

\section{Всё не так просто!}\label{---}

В реальности всё гораздо сложнее и посчитать апостериорную плотность
явно практически никогда не получается. Вместо явного вывода громоздкой
многомерной плотности мы будем хранить в памяти компьютера большую
выборку из апостериорного распределения, \(\theta^{(1)}\),
\(\theta^{(2)}\), \ldots.

И, например, если мы захотим посчитать вероятность
\(P(p>0.5 \mid \text{Данные})\), то мы просто посмотрим, какой процент
\(p^{(i)}\) из нашей апостериорной выборки оказался больше \(0.5\).

Отметим, что размер выборки из апостериорного распределения никак не
связан с количеством наблюдений. У нас может быть всего три наблюдения и
выборка размера 100 тысяч из апостериорного распределения. Чем больше
будет размер выборки из апостериорного распределения, тем точнее мы
опишем апостериорный закон распределения.

Чтобы построить выборку из апостериорного закона распределения
используют разные алгоритмы. Например, алгоритм Гиббса, алгоритм
Метрополиса-Гастингса или Гамильтоновское Монте-Карло.

\section{Алгоритм Гиббса}\label{-}

Допустим у нас три параметра, которые мы хотим оценить,
\(\theta = (\theta_1, \theta_2, \theta_3)\).

На выходе из алгоритма Гиббса мы хотим получить выборку из
апостериорного закона распределения.

Шаг 1. Выбираем произвольные допустимые стартовые значения
\(\theta_1^{(1)}\), \(\theta_2^{(1)}\), \(\theta_3^{(1)}\). Пусть будет
\(\theta_1^{(1)} = 0\), \(\theta_2^{(1)} = 0\), \(\theta_3^{(1)}=0\).

Шаг 2. По очереди обновляем каждый из параметров, используя самые свежии
версии остальных параметров.

Шаг 2.1. Случайно генерируем \(\theta_1^{(i+1)}\) из условного
распределения
\(f(\theta_1|\theta_2 = \theta_2^{(i)}, \theta_3 = \theta_3^{(i)}, \text{Данные})\).

Шаг 2.2. Случайно генерируем \(\theta_2^{(i+1)}\) из условного
распределения
\(f(\theta_2|\theta_1 = \theta_1^{(i+1)}, \theta_3 = \theta_3^{(i)}, \text{Данные})\).

Шаг 2.3. Случайно генерируем \(\theta_3^{(i+1)}\) из условного
распределения
\(f(\theta_3|\theta_1 = \theta_1^{(i+1)}, \theta_2 = \theta_2^{(i+1)}, \text{Данные})\).

Шаг 3. Пока не получилась достаточно большая выборка переходим к шагу 2.

Упражнения 1. Историческая задача от сэра Томаса Байеса

Два игрока, А и Б, играют друг с другом. Ничья невозможна. Изначально
считаем, что вероятность выигрыша игрока А равна \(p\) и она
распределена равномерна на \([0;1]\). Игрок А выиграл пять партий у
игрока Б из пяти сыгранных.

\begin{itemize}
\tightlist
\item
  Найдите апостериорную плотность \(p\);
\item
  Найдите апостериорную и априорную вероятность того, что игрок А
  выиграет следующую партию?
\item
  Восхититесь Байесом, который решал эту задачу не зная интегралов и на
  старо-английском :)
\end{itemize}

Упражнение 2.

Для апостериорной плотности \[
f(p \mid \text{Данные }) =
\begin{cases}
12p^2 (1-p), \text{ если } p \in [0;1] \\
0, \text{ иначе}
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Найдите правую границу байесовского доверительного HPD-интервала,
  начинающегося в точке \(p=0.1\);
\item
  Какой уровень доверия у этого байесовского интервала?
\end{itemize}

\section{Алгоритм Метрополиса-Гастингса}\label{--}


\end{document}
